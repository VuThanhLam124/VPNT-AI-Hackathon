{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c2796aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xóa VPNT-AI-Hackathon\n",
    "!rm -rf VPNT-AI-Hackathon\n",
    "# xóa kb_bge_m3_index.pkl\n",
    "!rm -rf kb_bge_m3_index.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc5f8b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "300b511f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'VPNT-AI-Hackathon'...\n",
      "remote: Enumerating objects: 28, done.\u001b[K\n",
      "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
      "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
      "remote: Total 28 (delta 1), reused 25 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (28/28), 16.14 MiB | 39.82 MiB/s, done.\n",
      "Resolving deltas: 100% (1/1), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/VuThanhLam124/VPNT-AI-Hackathon.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b8a305c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mVPNT-AI-Hackathon\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "000008ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/VPNT-AI-Hackathon\n"
     ]
    }
   ],
   "source": [
    "cd VPNT-AI-Hackathon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e013ee15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mAInicorns_TheBuilder_public_v1.1\u001b[0m/  kb_vnpt_embedding_index.pkl\n",
      "api_client.py                      LICENSE\n",
      "build_index.py                     predict.py\n",
      "data_utils.py                      README.md\n",
      "Dockerfile                         requirements.txt\n",
      "generate_simple.py                 rule.txt\n",
      "Huong_dan_nop_bai_Track2.txt       Tai_lieu_API_LLM_Embedding.txt\n",
      "\u001b[01;32minference.sh\u001b[0m*                      test.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab163916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 15 10:27:54 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   38C    P0             26W /  250W |       0MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# check GPU status\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13189c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m128.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m123.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mFound JSONL files:\n",
      "- /kaggle/input/data-vnpt-ai/atomic_weights.jsonl\n",
      "- /kaggle/input/data-vnpt-ai/diali_converted.jsonl\n",
      "- /kaggle/input/data-vnpt-ai/giaothong1_converted.jsonl\n",
      "- /kaggle/input/data-vnpt-ai/ho-chi-minh_data.jsonl\n",
      "- /kaggle/input/data-vnpt-ai/law1_converted.jsonl\n",
      "- /kaggle/input/data-vnpt-ai/law2_converted.jsonl\n",
      "- /kaggle/input/data-vnpt-ai/lichsu1_converted.jsonl\n",
      "- /kaggle/input/data-vnpt-ai/stem.jsonl\n",
      "- /kaggle/input/data-vnpt-ai/temple.jsonl\n",
      "- /kaggle/input/data-vnpt-ai/thue_converted.jsonl\n",
      "- /kaggle/input/data-vnpt-ai/tinh_converted.jsonl\n",
      "- /kaggle/input/data-vnpt-ai/unesco_vietnam.jsonl\n",
      "- /kaggle/input/data-vnpt-ai/vanhoa1_converted.jsonl\n",
      "- /kaggle/input/data-vnpt-ai/ward.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb30985660ac41a48faf970148befb1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading JSONL:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs: 4635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 09:45:22.498851: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765791922.663636     115 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765791922.712431     115 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGE device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7fc3f59ce614dfb89d1b27bea5c38e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e332ba976e3c493487fd468f9c7c0e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a0b85b780c41d683552aa80a72a295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc15e9c38fb442d91d713d6b172bac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5bb1f3fc6e4d77be4dec287bd15624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "705a13adc2ee46e78a279e96981d7b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6485d9f6a24c74b4b6f96b644dce9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af4a37f6a094120a41cd3ac1b252099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "960e7cc0835a4bd3a96bab048e2613dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f88b24ce88b41359318f9730eda670b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e145d2f6ec0b4b68870ccc3407302645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa50a6c2ca7947cd90fa3c720ff00026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e465a45fcc8244c7979de5bf3d643d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: (4635, 1024) float32\n",
      "Saved: /kaggle/working/kb_bge_m3_index.pkl\n",
      "Load/run Qwen3-8B failed: OSError Qwen/Qwen3-8B-Instruct is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission \n"
     ]
    }
   ],
   "source": [
    "# Kaggle: embed JSONL bằng BAAI/bge-m3 và (tuỳ chọn) load Qwen3-8B để inference local.\n",
    "# Lưu ý: cần bật Internet trong Kaggle để download model từ Hugging Face (hoặc upload weights như Kaggle Dataset).\n",
    "\n",
    "!pip -q install -U sentence-transformers transformers accelerate bitsandbytes\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterator, List\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "JSONL_ROOT = Path(\"/kaggle/input\")\n",
    "BGE_MODEL_ID = \"BAAI/bge-m3\"\n",
    "MAX_CHUNK_CHARS = 800\n",
    "MIN_TEXT_CHARS = 20\n",
    "LIMIT_DOCS = 0  # 0 = không giới hạn\n",
    "BGE_BATCH_SIZE = int(os.getenv(\"BGE_BATCH_SIZE\", \"32\"))\n",
    "OUT_PATH = Path(\"/kaggle/working/kb_bge_m3_index.pkl\")\n",
    "\n",
    "jsonl_paths = sorted(JSONL_ROOT.rglob(\"*.jsonl\"))\n",
    "if not jsonl_paths:\n",
    "    raise FileNotFoundError(f\"Không tìm thấy *.jsonl trong {JSONL_ROOT}\")\n",
    "\n",
    "print(\"Found JSONL files:\")\n",
    "for p in jsonl_paths[:50]:\n",
    "    print(\"-\", p)\n",
    "if len(jsonl_paths) > 50:\n",
    "    print(f\"... (+{len(jsonl_paths) - 50} files)\")\n",
    "\n",
    "\n",
    "def iter_json_objects_from_mixed_jsonl(path: Path) -> Iterator[Dict]:\n",
    "    raw = path.read_text(encoding=\"utf-8-sig\")\n",
    "\n",
    "    # 1) Try full JSON array/object\n",
    "    try:\n",
    "        s = raw.strip()\n",
    "        if s.startswith(\"[\") or s.startswith(\"{\"):\n",
    "            obj = json.loads(s)\n",
    "            if isinstance(obj, list):\n",
    "                for it in obj:\n",
    "                    if isinstance(it, dict):\n",
    "                        yield it\n",
    "                return\n",
    "            if isinstance(obj, dict):\n",
    "                yield obj\n",
    "                return\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) Try JSONL line-by-line (allow trailing comma)\n",
    "    yielded_any = False\n",
    "    for line in raw.splitlines():\n",
    "        s = line.strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        s = s.rstrip(\"\\\\\").rstrip()\n",
    "        if s.endswith(\",\"):\n",
    "            s = s[:-1].rstrip()\n",
    "        try:\n",
    "            obj = json.loads(s)\n",
    "        except Exception:\n",
    "            continue\n",
    "        if isinstance(obj, dict):\n",
    "            yielded_any = True\n",
    "            yield obj\n",
    "    if yielded_any:\n",
    "        return\n",
    "\n",
    "    # 3) Fallback: sequential JSON decoder\n",
    "    decoder = json.JSONDecoder()\n",
    "    i = 0\n",
    "    n = len(raw)\n",
    "    while i < n:\n",
    "        while i < n and raw[i] in \" \\t\\r\\n,\\\\\":\n",
    "            i += 1\n",
    "        if i >= n:\n",
    "            break\n",
    "        if raw[i] not in \"[{\":\n",
    "            i += 1\n",
    "            continue\n",
    "        try:\n",
    "            obj, end = decoder.raw_decode(raw, i)\n",
    "        except Exception:\n",
    "            i += 1\n",
    "            continue\n",
    "        i = end\n",
    "        if isinstance(obj, dict):\n",
    "            yield obj\n",
    "        elif isinstance(obj, list):\n",
    "            for it in obj:\n",
    "                if isinstance(it, dict):\n",
    "                    yield it\n",
    "\n",
    "\n",
    "def to_text(obj: Dict) -> str:\n",
    "    if not isinstance(obj, dict):\n",
    "        return \"\"\n",
    "    if isinstance(obj.get(\"text\"), str) and obj.get(\"text\").strip():\n",
    "        return obj[\"text\"].strip()\n",
    "    if \"question\" in obj and \"answer\" in obj:\n",
    "        q = str(obj.get(\"question\") or \"\").strip()\n",
    "        a = str(obj.get(\"answer\") or \"\").strip()\n",
    "        if q:\n",
    "            return f\"Hỏi: {q}\\nĐáp: {a}\".strip()\n",
    "    # fallback: key-value summary\n",
    "    parts = []\n",
    "    for k, v in obj.items():\n",
    "        if v is None:\n",
    "            continue\n",
    "        vs = str(v).strip()\n",
    "        if not vs:\n",
    "            continue\n",
    "        parts.append(f\"{k}: {vs}\")\n",
    "    return \". \".join(parts).strip()\n",
    "\n",
    "\n",
    "docs: List[Dict] = []\n",
    "texts: List[str] = []\n",
    "\n",
    "for path in tqdm(jsonl_paths, desc=\"Loading JSONL\"):\n",
    "    src = str(path)\n",
    "    for obj in iter_json_objects_from_mixed_jsonl(path):\n",
    "        t = to_text(obj)\n",
    "        if not t or len(t) < MIN_TEXT_CHARS:\n",
    "            continue\n",
    "        if MAX_CHUNK_CHARS and len(t) > MAX_CHUNK_CHARS:\n",
    "            t = t[:MAX_CHUNK_CHARS]\n",
    "        docs.append({\"text\": t, \"source\": src})\n",
    "        texts.append(t)\n",
    "        if LIMIT_DOCS and len(texts) >= LIMIT_DOCS:\n",
    "            break\n",
    "    if LIMIT_DOCS and len(texts) >= LIMIT_DOCS:\n",
    "        break\n",
    "\n",
    "print(\"Docs:\", len(docs))\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"BGE device:\", device)\n",
    "embedder = SentenceTransformer(BGE_MODEL_ID, device=device)\n",
    "try:\n",
    "    embedder.max_seq_length = 512\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "emb = embedder.encode(\n",
    "    texts,\n",
    "    batch_size=BGE_BATCH_SIZE,\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "emb = np.asarray(emb, dtype=np.float32)\n",
    "print(\"Embeddings:\", emb.shape, emb.dtype)\n",
    "\n",
    "with open(OUT_PATH, \"wb\") as f:\n",
    "    pickle.dump(\n",
    "        {\n",
    "            \"backend\": \"bge\",\n",
    "            \"model_name\": BGE_MODEL_ID,\n",
    "            \"docs\": docs,\n",
    "            \"embeddings\": emb,\n",
    "        },\n",
    "        f,\n",
    "    )\n",
    "print(\"Saved:\", OUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bee9ae15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84cdb03fb6cc44169892404d6f301ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da290a423da4f26b1e27d2e10115160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31818603650b4575889957de645422d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d16aebc3006467bba3e3ce8bf37f900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3080758d49fc4f1fbd1ba00ded0b812f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/729 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee31a5eb55e42a09420988f1811cb98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f57b0bfe821243d39d50a08188bb5160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c184f10211e64eac82ccb4d0f40662ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3655bf023aa44c4a854de96ec7f1a6b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86b0437f0d64f379f780e7e60dc45b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2796ec89f144d2a2537170b534a620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/1.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a9104168454f09be86cae595a77ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/3.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed53ba9cad74fbc86746eaa6adcb677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e3414c847043398986f545a7d7a483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: Qwen/Qwen3-8B-Base\n",
      "Khỉ Rhesus\n"
     ]
    }
   ],
   "source": [
    "USE_QWEN = True\n",
    "QWEN_MODEL_ID = \"Qwen/Qwen3-8B-Base\"\n",
    "\n",
    "if USE_QWEN:\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA chưa sẵn sàng -> bỏ qua load Qwen3-8B\")\n",
    "    else:\n",
    "        bnb_cfg = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "        )\n",
    "        try:\n",
    "            tok = AutoTokenizer.from_pretrained(QWEN_MODEL_ID, trust_remote_code=True)\n",
    "            llm = AutoModelForCausalLM.from_pretrained(\n",
    "                QWEN_MODEL_ID,\n",
    "                device_map=\"auto\",\n",
    "                quantization_config=bnb_cfg,\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "            llm.eval()\n",
    "            print(\"Loaded:\", QWEN_MODEL_ID)\n",
    "\n",
    "            # demo: hỏi 1 câu mẫu\n",
    "            sample_q = \"Khỉ nào được dùng phổ biến nhất để sản xuất vắc-xin bại liệt ở Việt Nam?\"\n",
    "            qv = embedder.encode([sample_q], batch_size=1, show_progress_bar=False, normalize_embeddings=True)\n",
    "            qv = np.asarray(qv, dtype=np.float32)[0]\n",
    "            sims = emb @ qv\n",
    "            top_idx = np.argsort(-sims)[:4]\n",
    "            ctx = \"\\n\\n---\\n\\n\".join(docs[int(i)][\"text\"] for i in top_idx)\n",
    "\n",
    "            prompt = (\n",
    "                \"Bạn là hệ thống trả lời trắc nghiệm. Chỉ trả về đáp án ngắn gọn.\\n\"\n",
    "                \"Dựa trên context, trả lời câu hỏi.\\n\\n\"\n",
    "                f\"<context>\\n{ctx}\\n</context>\\n\\n\"\n",
    "                f\"<question>\\n{sample_q}\\n</question>\\n\\n\"\n",
    "                \"Trả lời:\"\n",
    "            )\n",
    "            inputs = tok(prompt, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(llm.device) for k, v in inputs.items()}\n",
    "            out = llm.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=64,\n",
    "                do_sample=False,\n",
    "                temperature=0.0,\n",
    "            )\n",
    "            ans = tok.decode(out[0], skip_special_tokens=True)\n",
    "            print(ans[len(prompt):].strip())\n",
    "        except Exception as e:\n",
    "            print(\"Load/run Qwen3-8B failed:\", type(e).__name__, str(e)[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49581f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-8B-Base\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# prepare the model input\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switch between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
